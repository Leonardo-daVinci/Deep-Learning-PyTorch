{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwBbJe/JnZuSTzsT7hHiuw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leonardo-daVinci/Deep-Learning-PyTorch/blob/Intro-to-Neural-Networks/Intro_to_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lENpKSOzLjMC",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "1.   Neural Networks(NN) are at the heart of Deep Learning which mimic our brain.\n",
        "2.   These NNs comprise of various nodes and edges which take in input data and generate some output just like neurons firing. \n",
        "3.   The basic job of NN is to seperate given data points into different classes.\n",
        "\n",
        "##Classification Problem\n",
        "\n",
        "Let us suppose we have the problem of classifying student into accepted and not accepted by some university, we consider the following points.\n",
        "\n",
        "1.   The features such as marks obtained in tests and grades of the student.\n",
        "2.   Based of these features, identifying the line that seperates these two classes.\n",
        "\n",
        "In this case we have 2 features x1 (test score) and x2 (grade), so we can form a line  \n",
        "> w1x1 + w2x2 + b = 0\n",
        "\n",
        "if we have more features say n features then the seperation would be (n-1) dimentional i.e. 3 features would result in a plane that divides the data points into different classes. Thus for n features, the equation of (n-1) dimentional seperator would be  \n",
        "> w1x1 + w2x2 + .. + wnxn + b = 0\n",
        "\n",
        "The above equation can be converted into vector form as  \n",
        "> Wx + b = 0\n",
        "  \n",
        "where **W is (1xn)** vector, **x is (nx1)** vector and **b is (1x1)** vector  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnV8AAbzQXHw",
        "colab_type": "text"
      },
      "source": [
        "# Perceptron\n",
        "It is the building block of NN.  \n",
        "It takes in inputs corresponding to each feature and based on the funcion of our seperaor and outputs the result (y') in form of 0 or 1\n",
        "\n",
        "> y' = 0 if Wx + b < 0  \n",
        "> y' = 1 if Wx + b >= 0\n",
        "\n",
        "Thus we can define a perceptron as a combination of 2 components : \n",
        "\n",
        "1.   Linear Function that calculates value of Wx + b\n",
        "2.   Step Function which outputs 0 or 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j7YGZTLGkXL",
        "colab_type": "text"
      },
      "source": [
        "## Perceptrons as Logical Operators\n",
        "\n",
        "Since perceptrons take inputs and give outputs based on the perceptron function, it can be remodelled as logical operators.\n",
        "\n",
        "1.  **AND Perceptron**  \n",
        "And operator can be seen as a perceptron with two inputs which outputs 1 as result if both of them are 1, else output is 0.  \n",
        "By carefully calibrating weights and bias of perceptron, we can remodel it into AND operator.   \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlY602QeGofV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "1d2f3ad2-6735-41cb-d396-3da25fae93ec"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Note that different set of weights and bias might also work\n",
        "# Change the 3 values below to see if they work.\n",
        "weight1 = 1.0\n",
        "weight2 = 1.0\n",
        "bias = -1.5\n",
        "\n",
        "# Inputs and outputs\n",
        "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "correct_outputs = [False, False, False, True]\n",
        "outputs = []\n",
        "\n",
        "# Generate and check output\n",
        "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
        "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
        "    output = int(linear_combination >= 0)\n",
        "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
        "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
        "\n",
        "# Print output\n",
        "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
        "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
        "if not num_wrong:\n",
        "    print('Nice!  You got it all correct.\\n')\n",
        "else:\n",
        "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
        "print(output_frame.to_string(index=False))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nice!  You got it all correct.\n",
            "\n",
            " Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
            "       0          0                  -1.5                    0          Yes\n",
            "       0          1                  -0.5                    0          Yes\n",
            "       1          0                  -0.5                    0          Yes\n",
            "       1          1                   0.5                    1          Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_bnDf4aO4cg",
        "colab_type": "text"
      },
      "source": [
        "2.  **OR Perceptron**  \n",
        "Similar to the AND perceptron, OR perceptron can be generated using increased weights and decreased magnitude of bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rE1UNvMPk_N",
        "colab_type": "text"
      },
      "source": [
        "3.  **NOT Perceptron**  \n",
        "This type of perceptron only cares about one input and ignores the second input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YFpHdkBSs2c",
        "colab_type": "text"
      },
      "source": [
        "## Perceptron Algorithm \n",
        "For a point with coordinates (p,q), label y, learning rate α and prediction given by the equation:  \n",
        ">  y' = step(w1x1 + w2x2 + b)\n",
        "\n",
        "1.  If the point is correctly classified, do nothing.   \n",
        "2.  If the point is classified positive, but it has a negative label, subtract αp, αq, and α from w1, w2 and b respectively.\n",
        "3.  If the point is classified negative, but it has a positive label, add αp, αq, and α from w1, w2 and b respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwDyf4DUUKiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Setting the random seed, feel free to change it and see different solutions.\n",
        "np.random.seed(42)\n",
        "\n",
        "def stepFunction(t):\n",
        "    if t >= 0:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def prediction(X, W, b):\n",
        "    return stepFunction((np.matmul(X,W)+b)[0])\n",
        "\n",
        "# The function receives inputs as data X, the labels y,\n",
        "# the weights W (as an array), and the bias b,\n",
        "# We update the weights and bias W, b, according to the perceptron algorithm,\n",
        "# and return W and b.\n",
        "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
        "    for i in range(len(X)):\n",
        "        y_hat = prediction(X[i],W,b)\n",
        "        if y[i]-y_hat == 1:\n",
        "            W[0] += X[i][0]*learn_rate\n",
        "            W[1] += X[i][1]*learn_rate\n",
        "            b += learn_rate\n",
        "        elif y[i]-y_hat == -1:\n",
        "            W[0] -= X[i][0]*learn_rate\n",
        "            W[1] -= X[i][1]*learn_rate\n",
        "            b -= learn_rate\n",
        "    return W, b\n",
        "    \n",
        "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
        "# and returns a few of the boundary lines obtained in the iterations,\n",
        "# for plotting purposes.\n",
        "# Feel free to play with the learning rate and the num_epochs,\n",
        "# and see your results plotted below.\n",
        "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
        "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
        "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
        "    W = np.array(np.random.rand(2,1))\n",
        "    b = np.random.rand(1)[0] + x_max\n",
        "    # These are the solution lines that get plotted below.\n",
        "    boundary_lines = []\n",
        "    for i in range(num_epochs):\n",
        "        # In each epoch, we apply the perceptron step.\n",
        "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
        "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
        "    return boundary_lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmiAWoIi718M",
        "colab_type": "text"
      },
      "source": [
        "##Problems with Perceptron Algorithm\n",
        "As we can see above, our algorithm only outputs a straight line that can divide the classes.  \n",
        "If the boundary of these classes is in form of a curve, we need to modify the perceptron alogorithm so that it can found such non-linear boundaries."
      ]
    }
  ]
}