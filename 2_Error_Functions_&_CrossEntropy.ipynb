{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.Error_Functions_&_CrossEntropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLTIJKHXgE/unRZOkTVAF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leonardo-daVinci/Deep-Learning-PyTorch/blob/Error_Functions%26Cross_Entropy/2_Error_Functions_%26_CrossEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYo0qohHG72",
        "colab_type": "text"
      },
      "source": [
        "##Error Function\n",
        "\n",
        "It tells us how badly we are performing at the moment and how far we are from the solution.  \n",
        "In a cartezian plane, it may be the euclidean distance or some other distance between our current position and the target position.  \n",
        "Goal of the error function is to help us take steps towards our target position.  \n",
        "We take look around our current position and then find out the direction in which we are closest to the target position and take that step.  \n",
        "The process is repeated till we reach the target position.  \n",
        "The technique described above is called as **Gradient Descent**.\n",
        "  \n",
        "**Conditions to apply Gradient Descent :**\n",
        "\n",
        "1.   It should be continuous.\n",
        "2.   It should be differentiable.\n",
        "\n",
        "##Discrete vs Continuous Predictions\n",
        "\n",
        "Since we need a continuous error function, we need to move to continuous predictions.  \n",
        "For example if we predict the answer as yes or no, we need to change it to probability of yes or no, which is between 0 and 1.  \n",
        "Recall that we employed a step function in perceptron to give output as 0 or 1. To convert these discrete outputs to continuous ones, we replace the step function with **Sigmoid Function**.  \n",
        "\n",
        "> sigmoid(x) = 1/(1+exp(-x))\n",
        "\n",
        "Thus we can modify our activation function as follows:\n",
        "\n",
        "> y' = sigmoid(Wx + b)\n",
        "\n",
        "\n",
        "#Multi-Class Classification\n",
        "\n",
        "If there are more classes that our model can predict, such as given some fruits our model predicts whether it is banana, orange or apple, then such problem is called as **Multi-Class Clasification**.  \n",
        "  \n",
        "We determine scores for each class and then calculate probabilities. Since the scores are obtained as result of linear functions, they can be also be negative. So, to convert the negative scores into positive ones, we utilize exponential function.  \n",
        "  \n",
        "Thus we calculate the scores for each clss using a **Softmax function**.  Suppose there are N classes with Linear function scores as Z1, Z2, .. Zn then we define softmax function as:\n",
        "> P(class i) = exp(Zi)/sum(Z1+Z2..+Zn) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR2YbgY6aOEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# function takes input as list of numbers, and returns\n",
        "# the list of values given by the softmax function.\n",
        "def softmax(L):\n",
        "    result = []\n",
        "    deno = sum([np.exp(i) for i in L])\n",
        "    for i in L:\n",
        "       result.append(np.exp(i)/deno)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-cpIZCbTS1",
        "colab_type": "text"
      },
      "source": [
        "# One-Hot Encoding\n",
        "In our case of multiclass classification, to encode our classes, we define a variable for each class.  \n",
        "Suppose there are 3 classes C1, C2 and C3 and a particular example E belongs to class C2 then for C2 the value is 1 an for other classes it is zero.  \n",
        "That is, E would be encoded as [0,1,0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CXPYMdKdkKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ae8b8416-c0ae-4f01-e00f-0e3e4456345b"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "#here we have 3 classes so we will have 3 different variables after one-hot encoding\n",
        "X = [['Orange'],['Apple'],['Mango']]\n",
        "X = enc.fit_transform(X).toarray()\n",
        "X"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}