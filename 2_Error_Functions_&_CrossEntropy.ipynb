{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.Error_Functions_&_CrossEntropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnROgoNtGNqLJYpl4/AjK/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leonardo-daVinci/Deep-Learning-PyTorch/blob/Error_Functions%26Cross_Entropy/2_Error_Functions_%26_CrossEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYo0qohHG72",
        "colab_type": "text"
      },
      "source": [
        "##Error Function\n",
        "\n",
        "It tells us how badly we are performing at the moment and how far we are from the solution.  \n",
        "In a cartezian plane, it may be the euclidean distance or some other distance between our current position and the target position.  \n",
        "Goal of the error function is to help us take steps towards our target position.  \n",
        "We take look around our current position and then find out the direction in which we are closest to the target position and take that step.  \n",
        "The process is repeated till we reach the target position.  \n",
        "The technique described above is called as **Gradient Descent**.\n",
        "  \n",
        "**Conditions to apply Gradient Descent :**\n",
        "\n",
        "1.   It should be continuous.\n",
        "2.   It should be differentiable.\n",
        "\n",
        "##Discrete vs Continuous Predictions\n",
        "\n",
        "Since we need a continuous error function, we need to move to continuous predictions.  \n",
        "For example if we predict the answer as yes or no, we need to change it to probability of yes or no, which is between 0 and 1.  \n",
        "Recall that we employed a step function in perceptron to give output as 0 or 1. To convert these discrete outputs to continuous ones, we replace the step function with **Sigmoid Function**.  \n",
        "\n",
        "> sigmoid(x) = 1/(1+exp(-x))\n",
        "\n",
        "Thus we can modify our activation function as follows:\n",
        "\n",
        "> y' = sigmoid(Wx + b)\n",
        "\n",
        "\n",
        "#Multi-Class Classification\n",
        "\n",
        "If there are more classes that our model can predict, such as given some fruits our model predicts whether it is banana, orange or apple, then such problem is called as **Multi-Class Clasification**.  \n",
        "  \n",
        "We determine scores for each class and then calculate probabilities. Since the scores are obtained as result of linear functions, they can be also be negative. So, to convert the negative scores into positive ones, we utilize exponential function.  \n",
        "  \n",
        "Thus we calculate the scores for each clss using a **Softmax function**.  Suppose there are N classes with Linear function scores as Z1, Z2, .. Zn then we define softmax function as:\n",
        "> P(class i) = exp(Zi)/sum(Z1+Z2..+Zn) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR2YbgY6aOEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# function takes input as list of numbers, and returns\n",
        "# the list of values given by the softmax function.\n",
        "def softmax(L):\n",
        "    result = []\n",
        "    deno = sum([np.exp(i) for i in L])\n",
        "    for i in L:\n",
        "       result.append(np.exp(i)/deno)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-cpIZCbTS1",
        "colab_type": "text"
      },
      "source": [
        "## One-Hot Encoding\n",
        "In our case of multiclass classification, to encode our classes, we define a variable for each class.  \n",
        "Suppose there are 3 classes C1, C2 and C3 and a particular example E belongs to class C2 then for C2 the value is 1 an for other classes it is zero.  \n",
        "That is, E would be encoded as [0,1,0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CXPYMdKdkKI",
        "colab_type": "code",
        "outputId": "ae8b8416-c0ae-4f01-e00f-0e3e4456345b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "#here we have 3 classes so we will have 3 different variables after one-hot encoding\n",
        "X = [['Orange'],['Apple'],['Mango']]\n",
        "X = enc.fit_transform(X).toarray()\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htBo-ELtTStd",
        "colab_type": "text"
      },
      "source": [
        "##Maximum Likelihood\n",
        "This method involves picking up the model that gives the existing labels the highest probability.  \n",
        "Thus by maximizing the probability, we can pick up the best possible model.  \n",
        "Let us consider an example where we have 2 models that classify 4 points into blue and red.\n",
        "\n",
        "1.   First model M1, clasifies 2 points correctly while other two points incorrectly. \n",
        "2.   Second model M2, classifies all the points correctly.\n",
        "\n",
        "Let us say that the probabilities of correctly classifying a point for the models are as follows: \n",
        "\n",
        "1.   For M1, probabilities are 0.60, 0.7, 0.2 and 0.1 (2 correctly classified points).\n",
        "2.   For M2, probabilities are 0.7, 0.9, 0.6 and 0.8 (all points correctly classified).\n",
        "\n",
        "Now, product of probabilities for M1 is 0.0084 while for M2 it is 0.3024, which is significantly higher than M1. Thus by method of Maximum Likelihood we can confirm that model M2 is better than M1.  \n",
        "  \n",
        "##Cross-Entropy\n",
        "We use the sum of probabilities rather than their products because of the following reasons:\n",
        "\n",
        "1.   If there are thousands of data points then the products of their probabilities would be very very small.\n",
        "2.   A change in probability of even one data point can drastically change the entire product.\n",
        "\n",
        "So to solve the above issues, we use logarithm function to change product of probabilities into sum of probabilities.  \n",
        "Now, the probabilities are between 0 and 1, thus we have their logarithms as negative values. To turn the sum into positive, we calculate the negative logarithm. This is called as **Cross-Entropy**.  \n",
        "\n",
        "It is also known that logarithms of numbers close to one are smaller as compared to numbers closer to zero. Thus a **good model will give us low cross-entropy while a bad model will give us high cross-entropy**.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqMkZCw1fJ0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# cross_entropy function takes 2 input - lists Y, P,\n",
        "# where Y is 0 or 1 depending upon point being correctly classified or not\n",
        "# and P is corresponding probabilty of that point\n",
        "# it returns the float corresponding to their cross-entropy.\n",
        "def cross_entropy(Y, P):\n",
        "    Y = np.float_(Y)\n",
        "    P = np.float_(P)\n",
        "    return -sum(Y*np.log(P) + (1-Y)*np.log(1-P))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}